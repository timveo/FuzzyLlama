# AI Monitoring Configuration Template
#
# PURPOSE: Define observability stack for AI services
# USAGE: AIOps Engineer customizes based on infrastructure

# =============================================================================
# PROMETHEUS METRICS
# =============================================================================

prometheus:
  namespace: ai_service

  # Custom metrics to expose
  metrics:
    # Request metrics
    - name: requests_total
      type: counter
      help: Total AI API requests
      labels: [model, status, task_type]

    - name: request_duration_seconds
      type: histogram
      help: Request latency distribution
      labels: [model]
      buckets: [0.1, 0.25, 0.5, 1, 2, 5, 10]

    - name: tokens_total
      type: counter
      help: Total tokens processed
      labels: [model, direction]  # direction: input/output

    - name: cost_usd_total
      type: counter
      help: Cumulative cost in USD
      labels: [model]

    # Cache metrics
    - name: cache_hits_total
      type: counter
      help: Cache hit count
      labels: [cache_type]

    - name: cache_misses_total
      type: counter
      help: Cache miss count
      labels: [cache_type]

    # Error metrics
    - name: errors_total
      type: counter
      help: Error count by type
      labels: [model, error_type]

    # Queue metrics
    - name: queue_depth
      type: gauge
      help: Current queue depth
      labels: [priority]

    - name: queue_wait_seconds
      type: histogram
      help: Time spent waiting in queue
      buckets: [0.1, 0.5, 1, 5, 10, 30]

# =============================================================================
# ALERT RULES
# =============================================================================

alerts:
  groups:
    - name: ai_service_critical
      rules:
        - alert: AIServiceHighErrorRate
          expr: |
            (sum(rate(ai_service_errors_total[5m])) /
             sum(rate(ai_service_requests_total[5m]))) > 0.05
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: AI service error rate above 5%
            description: "Error rate is {{ $value | humanizePercentage }}"

        - alert: AIServiceDown
          expr: up{job="ai-service"} == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: AI service is down

        - alert: AIBudgetExceeded
          expr: increase(ai_service_cost_usd_total[1h]) > 50
          labels:
            severity: critical
          annotations:
            summary: Hourly AI spend exceeded $50

    - name: ai_service_warning
      rules:
        - alert: AIServiceHighLatency
          expr: |
            histogram_quantile(0.95,
              rate(ai_service_request_duration_seconds_bucket[5m])) > 2
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: P95 latency above 2 seconds

        - alert: AIServiceCacheHitLow
          expr: |
            (sum(rate(ai_service_cache_hits_total[1h])) /
             (sum(rate(ai_service_cache_hits_total[1h])) +
              sum(rate(ai_service_cache_misses_total[1h])))) < 0.3
          for: 1h
          labels:
            severity: warning
          annotations:
            summary: Cache hit rate below 30%

        - alert: AIServiceQueueBacklog
          expr: ai_service_queue_depth > 100
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: Request queue depth above 100

# =============================================================================
# GRAFANA DASHBOARDS
# =============================================================================

dashboards:
  - name: AI Service Overview
    uid: ai-service-overview
    panels:
      - title: Request Rate
        type: graph
        query: sum(rate(ai_service_requests_total[5m])) by (model)

      - title: Error Rate
        type: stat
        query: |
          sum(rate(ai_service_errors_total[5m])) /
          sum(rate(ai_service_requests_total[5m]))
        thresholds:
          - color: green
            value: 0
          - color: yellow
            value: 0.01
          - color: red
            value: 0.05

      - title: P95 Latency
        type: graph
        query: |
          histogram_quantile(0.95,
            sum(rate(ai_service_request_duration_seconds_bucket[5m])) by (le, model))

      - title: Token Usage
        type: graph
        query: sum(rate(ai_service_tokens_total[1h])) by (model, direction)

      - title: Cost (Last 24h)
        type: stat
        query: increase(ai_service_cost_usd_total[24h])
        unit: currencyUSD

      - title: Cache Hit Rate
        type: gauge
        query: |
          sum(rate(ai_service_cache_hits_total[1h])) /
          (sum(rate(ai_service_cache_hits_total[1h])) +
           sum(rate(ai_service_cache_misses_total[1h])))
        thresholds:
          - color: red
            value: 0
          - color: yellow
            value: 0.5
          - color: green
            value: 0.7

# =============================================================================
# LOGGING
# =============================================================================

logging:
  level: info
  format: json

  # Structured log fields
  fields:
    - request_id
    - model
    - task_type
    - latency_ms
    - tokens_in
    - tokens_out
    - cost_usd
    - cache_hit
    - error_type

  # Sampling for high-volume logs
  sampling:
    enabled: true
    rate: 0.1  # Log 10% of successful requests
    always_log:
      - errors
      - slow_requests  # > 2s
      - high_cost      # > $0.10

# =============================================================================
# TRACING
# =============================================================================

tracing:
  enabled: true
  provider: opentelemetry

  # Span configuration
  spans:
    - name: ai_request
      attributes:
        - model
        - task_type
        - prompt_length
        - response_length

    - name: cache_lookup
      attributes:
        - cache_key
        - hit

    - name: model_inference
      attributes:
        - model
        - tokens_in
        - tokens_out

  # Sampling
  sampling:
    type: probabilistic
    rate: 0.1  # Sample 10% of traces

# =============================================================================
# HEALTH CHECKS
# =============================================================================

health_checks:
  - name: model_reachable
    type: probe
    endpoint: /health/model
    interval: 30s
    timeout: 10s

  - name: cache_healthy
    type: probe
    endpoint: /health/cache
    interval: 30s
    timeout: 5s

  - name: queue_healthy
    type: probe
    endpoint: /health/queue
    interval: 30s
    timeout: 5s

# =============================================================================
# RUNBOOK LINKS
# =============================================================================

runbooks:
  AIServiceHighErrorRate: docs/runbooks/ai-high-error-rate.md
  AIServiceDown: docs/runbooks/ai-service-down.md
  AIServiceHighLatency: docs/runbooks/ai-high-latency.md
  AIBudgetExceeded: docs/runbooks/ai-budget-exceeded.md
